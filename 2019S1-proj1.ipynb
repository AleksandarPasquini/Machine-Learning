{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s):\n",
    "###### Python version:\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(filename, header):\n",
    "    dataset = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile, header)\n",
    "        for row in reader:\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(dataset, header, classlabels):\n",
    "    model = []\n",
    "    smoothing = []\n",
    "    classes = {}\n",
    "    count = 0\n",
    "    for index in range(len(dataset)):\n",
    "        count += 1;\n",
    "        if dataset[index]['class'] in classes:\n",
    "            classes[dataset[index]['class']] += 1\n",
    "        else:\n",
    "            classes[dataset[index]['class']] = 1\n",
    "    classes['count'] = count\n",
    "    model.append(classes)\n",
    "    \n",
    "    #Laplace smoothing\n",
    "    j = 0\n",
    "    while j < len(header)-1:\n",
    "        attributes = {}\n",
    "        count = 0\n",
    "        for index in range(len(dataset)):\n",
    "            if dataset[index][header[j]] not in attributes:\n",
    "                count += 1\n",
    "                attributes[dataset[index][header[j]]] = 1\n",
    "        j+=1\n",
    "        attributes['count'] = count\n",
    "        smoothing.append(attributes)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(classlabels):\n",
    "        current_class = classlabels[i]\n",
    "        j = 0\n",
    "        class_attributes = []\n",
    "        while j < len(header)-1:\n",
    "            attributes = smoothing[j].copy()\n",
    "            for index in range(len(dataset)):\n",
    "                if dataset[index]['class'] == current_class:\n",
    "                    attributes['count'] += 1\n",
    "                    attributes[dataset[index].get(header[j])] += 1\n",
    "            class_attributes.append(attributes)\n",
    "            j+=1\n",
    "        model.append(class_attributes)\n",
    "        i+=1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(model, dataset, classlabels, header):\n",
    "    predicted_classes = []\n",
    "    highest_probability = 0\n",
    "    probability = 0\n",
    "    for index in range(len(dataset)):\n",
    "        row = dataset[index]\n",
    "        highest_probability = 0\n",
    "        probability = 0\n",
    "        predicted_label = ''\n",
    "        for label in classlabels:\n",
    "            probability = model[0][label]/model[0]['count']\n",
    "            for attribute in range(len(row)-1):\n",
    "                probability *= model[classlabels.index(label)+1][attribute][row.get(header[attribute])]/model[classlabels.index(label)+1][attribute]['count']\n",
    "            if probability > highest_probability:\n",
    "                highest_probability = probability\n",
    "                predicted_label = label\n",
    "        predicted_classes.append(predicted_label)\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(predicted_classes, dataset):\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    count = 0\n",
    "    for index in range(len(dataset)):\n",
    "        row = dataset[index]\n",
    "        if predicted_classes[index] == row['class']:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        count += 1\n",
    "    print(\"Accuracy is\", correct/count*100,\"% with\", correct,\"correct predictions and\", wrong, \"wrong predictions\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_info(model, classlabels):\n",
    "    classes = model[0]\n",
    "    entropy_classes = 0\n",
    "    i = 0\n",
    "    while i < len(classes)-1:\n",
    "        entropy_classes += classes[classlabels[i]]/classes['count']*math.log(classes[classlabels[i]]/classes['count'],2)\n",
    "        i += 1\n",
    "    entropy_classes = entropy_classes*(-1)\n",
    "    return entropy_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_info(dataset, attribute):\n",
    "    attributes = {}\n",
    "    count = 0\n",
    "    for index in range(len(dataset)):\n",
    "        count += 1;\n",
    "        if dataset[index][attribute] in attributes:\n",
    "            attributes[dataset[index][attribute]] += 1\n",
    "        else:\n",
    "            attributes[dataset[index][attribute]] = 1\n",
    "    attributes['count'] = count\n",
    "    entropy_attributes = 0\n",
    "    i = 0\n",
    "    for value in attributes.keys():\n",
    "        if value != 'count':\n",
    "            entropy_attributes += attributes[value]/attributes['count']*math.log(attributes[value]/attributes['count'],2)\n",
    "    entropy_attributes = entropy_attributes*(-1)\n",
    "    return entropy_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_info(dataset, header, model, classlabels):\n",
    "    j = 0\n",
    "    attributes_values = []\n",
    "    while j < len(header)-1:\n",
    "        count = 0\n",
    "        attributes = []\n",
    "        for index in range(len(dataset)):\n",
    "            if dataset[index][header[j]] not in attributes:\n",
    "                attributes.append(dataset[index][header[j]])\n",
    "        j+=1  \n",
    "        attributes_values.append(attributes)\n",
    "    attribute_index = 0\n",
    "    entropy_attributes = []\n",
    "    print(attributes_values)\n",
    "    for attribute_index in  range(len(header)-1):\n",
    "        mean_info = 0\n",
    "        for value in attributes_values[attribute_index]:\n",
    "            i = 1\n",
    "            j = 1\n",
    "            total = 0\n",
    "            entropy = 0\n",
    "            while i <= len(classlabels):\n",
    "                print(\"model\",model[i][attribute_index][value])\n",
    "                total += model[i][attribute_index][value]-1\n",
    "                i += 1\n",
    "            print(total)\n",
    "            while j <= len(classlabels):\n",
    "                if (model[j][attribute_index][value]-1)!= 0:\n",
    "                    entropy += (model[j][attribute_index][value]-1)/total*math.log((model[j][attribute_index][value]-1)/total,2)\n",
    "                j += 1\n",
    "            entropy_attribute = entropy*(-1)\n",
    "            mean_info += (total/model[0]['count'])*entropy_attribute\n",
    "        entropy_attributes.append(mean_info)\n",
    "    return entropy_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(attributes_info,class_info, header):\n",
    "    info_gain = []\n",
    "    i = 0\n",
    "    for value in attributes_info:\n",
    "        info_gain.append(class_info-value)\n",
    "    while i < len(header)-1:\n",
    "        print(\"information gain for\", header[i], \"is\", info_gain[i])\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 90.30092592592592 % with 11703 correct predictions and 1257 wrong predictions\n",
      "1.7164959001837932\n",
      "[['usual', 'pretentious', 'great_pret'], ['proper', 'less_proper', 'improper', 'critical', 'very_crit'], ['complete', 'completed', 'incomplete', 'foster'], ['1', '2', '3', 'more'], ['convenient', 'less_conv', 'critical'], ['convenient', 'inconv'], ['nonprob', 'slightly_prob', 'problematic'], ['recommended', 'priority', 'not_recom']]\n",
      "model 1441\n",
      "model 3\n",
      "model 197\n",
      "model 1925\n",
      "model 759\n",
      "4320\n",
      "model 1441\n",
      "model 1\n",
      "model 133\n",
      "model 1485\n",
      "model 1265\n",
      "4320\n",
      "model 1441\n",
      "model 1\n",
      "model 1\n",
      "model 859\n",
      "model 2023\n",
      "4320\n",
      "model 865\n",
      "model 3\n",
      "model 131\n",
      "model 1345\n",
      "model 253\n",
      "2592\n",
      "model 865\n",
      "model 1\n",
      "model 133\n",
      "model 1345\n",
      "model 253\n",
      "2592\n",
      "model 865\n",
      "model 1\n",
      "model 67\n",
      "model 905\n",
      "model 759\n",
      "2592\n",
      "model 865\n",
      "model 1\n",
      "model 1\n",
      "model 465\n",
      "model 1265\n",
      "2592\n",
      "model 865\n",
      "model 1\n",
      "model 1\n",
      "model 211\n",
      "model 1519\n",
      "2592\n",
      "model 1081\n",
      "model 3\n",
      "model 119\n",
      "model 1153\n",
      "model 889\n",
      "3240\n",
      "model 1081\n",
      "model 1\n",
      "model 101\n",
      "model 1093\n",
      "model 969\n",
      "3240\n",
      "model 1081\n",
      "model 1\n",
      "model 71\n",
      "model 1039\n",
      "model 1053\n",
      "3240\n",
      "model 1081\n",
      "model 1\n",
      "model 41\n",
      "model 985\n",
      "model 1137\n",
      "3240\n",
      "model 1081\n",
      "model 3\n",
      "model 149\n",
      "model 1207\n",
      "model 805\n",
      "3240\n",
      "model 1081\n",
      "model 1\n",
      "model 101\n",
      "model 1093\n",
      "model 969\n",
      "3240\n",
      "model 1081\n",
      "model 1\n",
      "model 41\n",
      "model 985\n",
      "model 1137\n",
      "3240\n",
      "model 1081\n",
      "model 1\n",
      "model 41\n",
      "model 985\n",
      "model 1137\n",
      "3240\n",
      "model 1441\n",
      "model 3\n",
      "model 209\n",
      "model 1619\n",
      "model 1053\n",
      "4320\n",
      "model 1441\n",
      "model 1\n",
      "model 101\n",
      "model 1397\n",
      "model 1385\n",
      "4320\n",
      "model 1441\n",
      "model 1\n",
      "model 21\n",
      "model 1253\n",
      "model 1609\n",
      "4320\n",
      "model 2161\n",
      "model 3\n",
      "model 219\n",
      "model 2245\n",
      "model 1857\n",
      "6480\n",
      "model 2161\n",
      "model 1\n",
      "model 111\n",
      "model 2023\n",
      "model 2189\n",
      "6480\n",
      "model 1441\n",
      "model 2\n",
      "model 165\n",
      "model 1516\n",
      "model 1201\n",
      "4320\n",
      "model 1441\n",
      "model 2\n",
      "model 165\n",
      "model 1516\n",
      "model 1201\n",
      "4320\n",
      "model 1441\n",
      "model 1\n",
      "model 1\n",
      "model 1237\n",
      "model 1645\n",
      "4320\n",
      "model 1\n",
      "model 3\n",
      "model 329\n",
      "model 2413\n",
      "model 1579\n",
      "4320\n",
      "model 1\n",
      "model 1\n",
      "model 1\n",
      "model 1855\n",
      "model 2467\n",
      "4320\n",
      "model 4321\n",
      "model 1\n",
      "model 1\n",
      "model 1\n",
      "model 1\n",
      "4320\n",
      "information gain for parents is 0.07293460750309921\n",
      "information gain for has_nurs is 0.1964492804881155\n",
      "information gain for form is 0.005572591715219621\n",
      "information gain for children is 0.011886431475775616\n",
      "information gain for housing is 0.01960202502287145\n",
      "information gain for finance is 0.0043331270252000564\n",
      "information gain for social is 0.022232616894017898\n",
      "information gain for health is 0.958774960469976\n",
      "[1.643561292680694, 1.5200466196956777, 1.7109233084685735, 1.7046094687080176, 1.6968938751609217, 1.712162773158593, 1.6942632832897753, 0.7577209397138172]\n",
      "information gain for parents is -0.05859879195953788\n",
      "information gain for has_nurs is 0.0649158810254784\n",
      "information gain for form is -0.12596080774741747\n",
      "information gain for children is -0.11964696798686147\n",
      "information gain for housing is -0.11193137443976564\n",
      "information gain for finance is -0.12720027243743703\n",
      "information gain for social is -0.10930078256861919\n",
      "information gain for health is 0.8272415610073389\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "#This changes depending on the file \n",
    "filename = 'nursery.csv'\n",
    "header = 'parents','has_nurs','form','children','housing','finance','social','health','class'\n",
    "classlabels = 'not_recom', 'recommend', 'very_recom', 'priority', 'spec_prior'\n",
    "\n",
    "\n",
    "\n",
    "dataset = preprocess(filename, header)\n",
    "model = train(dataset, header, classlabels)\n",
    "predicted_classes = predict(model,dataset,classlabels, header)\n",
    "evaluate(predicted_classes, dataset)\n",
    "class_info1 = class_info(model, classlabels)\n",
    "print(class_info1)\n",
    "attributes = ['parents','has_nurs','form,children','housing','finance','social','health']\n",
    "attributes_info1 = attributes_info(dataset, header, model, classlabels)\n",
    "info_gain1 = info_gain(attributes_info1,class_info1, header) \n",
    "print(attributes_info1)\n",
    "entropy_attribute = att_info(dataset, 'parents')\n",
    "print(info_gain(attributes_info1,entropy_attribute,header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.\"cmc\" Accuracy is 50.577053632043445 % with 745 correct predictions and 728 wrong predictions\n",
    "information gain for w-education is 0.07090633894894594\n",
    "information gain for h-education is 0.04013859922938412\n",
    "information gain for n-child is 0.10173991727554088\n",
    "information gain for w-relation is 0.009820501434385065\n",
    "information gain for w-work is 0.002582332379721608\n",
    "information gain for h-occupation is 0.030474214560266777\n",
    "information gain for standard-of-living is 0.032511460053806784\n",
    "information gain for media-exposure is 0.01578645559562042\n",
    "\n",
    "\"car\" Accuracy is 87.15277777777779 % with 1506 correct predictions and 222 wrong predictions\n",
    "information gain for buying is 0.09644896916961399\n",
    "information gain for maint is 0.07370394692148596\n",
    "information gain for doors is 0.004485716626632108\n",
    "information gain for persons is 0.2196629633399082\n",
    "information gain for lug_boot is 0.030008141247605424\n",
    "information gain for safety is 0.26218435655426386\n",
    "\n",
    "\"nursery\" Accuracy is 90.30092592592592 % with 11703 correct predictions and 1257 wrong predictions\n",
    "information gain for parents is 0.07293460750309921\n",
    "information gain for has_nurs is 0.1964492804881155\n",
    "information gain for form is 0.005572591715219621\n",
    "information gain for children is 0.011886431475775616\n",
    "information gain for housing is 0.01960202502287145\n",
    "information gain for finance is 0.0043331270252000564\n",
    "information gain for social is 0.022232616894017898\n",
    "information gain for health is 0.958774960469976\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
